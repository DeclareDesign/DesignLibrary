---
title: "Cluster Sampling"
output: rmarkdown::html_vignette
bibliography: bib.bib
nocite: | 
  @murray1998design  
vignette: >
  %\VignetteIndexEntry{Cluster Sampling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r MIDA, echo = FALSE,include = FALSE}
library(DesignLibrary)
library(ggplot2)
library(knitr)
```

Researchers often cannot randomly sample at the individual level because it may, among other reasons, be too costly or logistically impractical. Instead, they may choose to randomly sample households, political precincts, or any group of individuals in order to draw inferences about the population. This strategy may be cheaper and simpler but may also introduce risks of less precise estimates.

<!-- Include reference to simple random sampling vignette below -->
<!-- In *Simple Random Sampling*, we discused the example of estimating the average political ideology in Portola, California. Consider instead  -->

Say we are interested in the average party ideology in the entire state of California. Using cluster sampling, we randomly sample counties within the state, and within each selected county, randomly sample individuals to survey.

Assuming enough variation in the outcome of interest, random cluster assignment yields unbiased but imprecise estimates. By sampling clusters, we select groups of individuals who may share common attributes. Unlike simple random sampling, we need to take  account of this intra-cluster correlation in our estimation of the standard error.^[The intra-cluster correlation coefficient (ICC) can be calculated directly and is a feature of this design.] The higher the degree of within-cluster similarity, the more variance we observe in cluster-level averages and the more imprecise are our estimates.^[In ordinary least square (OLS) models, we assume errors are independent (error terms between individual observations are uncorrelated with each other) and homoskedastic (the size of errors is homogeneous across individuals). In reality, this is often not the case with cluster sampling.] We address this by considering cluster-robust standard errors in our answer strategy below.

## Design Declaration

- **M**odel: We specify the variable of interest $Y$ (political ideology, say) as a discrete variable ranging from 1 (most liberal) to 7 (most conservative). We do not define a functional model since we are interested in the population mean of $Y$. The model also includes information about the number of sampled clusters and the number of individuals per cluster.

- **I**nquiry: Our estimand is the population mean of political identification $Y$. Because we employed random sampling, we can expect the value of the sample mean ($\widehat{\overline{y}}$) to approximate the true population parameter ($\widehat{\overline{Y}}$).

- **D**ata strategy: sampling follows a two-stage strategy. We first draw a random sample 30 counties in California, and in each county select 20 individuals at random. This guarantees that each county has the same probability of being included in the sample and each resident within a county the same probability of being in the sample. In this design we estimate $Y$ for n = 600 respondents.

- **A**nswer strategy: We estimate the population mean with the sample mean estimator: $\widehat{\overline{Y}} = \frac{1}{n} \sum_1^n Y_i$, and estimate standard errors under the assumption of independent and heteroskedastic errors as well as cluster-robust standard errors to take into account correlation of errors within clusters. Below we demonstrate the the imprecision of our estimated $\widehat{\overline{Y}}$ when we cluster standard errors and when we do not in the presence of an intracluster correlation coefficient (ICC) of 0.402.


```{r,eval = TRUE, code = get_design_code(cluster_sampling_designer(n_clusters = 30,n_subjects_per_cluster = 20,icc = 0.402))}
```

## Takeaways

We now diagnose the design:

```{r}
diagnosis <- diagnose_design(cluster_sampling_design)
```

```{r,eval = TRUE, echo = FALSE}
kable(reshape_diagnosis(diagnosis)[, -c(1:2, 4:6)], digits = 2)
```

To appreciate the role of clustering better we also plot simulated values of our estimand with standard errors not clustered and with clustered standard errors. TO do this we first add an additional estimator to the design that does not take account of clusters. 

```{r plot, echo=TRUE, warning=FALSE}

new_design <- cluster_sampling_design + declare_estimator(Y ~ 1,
                                        model = lm_robust,
                                        estimand = estimand,
                                        label = "Naive Standard Errors")
diagnosis <- diagnose_design(new_design)
```

```{r plot, echo=FALSE, warning=FALSE}


sims <- diagnosis$simulations

dd_theme <-
  function() {
    theme_bw() +
      theme(
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        panel.border = element_blank(),
        panel.grid.major = element_line(color = '#eeeeee'),
        strip.background = element_blank(),
        legend.position = "bottom"
        # text = element_text(family = "Palatino")
        )
  }

  sims$covered <- factor(1 + (sims$conf.low < sims$estimand & sims$estimand < sims$conf.high), 1:2, labels = c("Estimand not covered by confidence interval", "Estimand covered by confidence interval"))
  sims$estimator_label <- as.factor(sims$estimator_label)
  sims$estimator_label <- factor(sims$estimator_label, levels = rev(levels(sims$estimator_label)))
  sims$estimand_label  <- as.factor(sims$estimand_label)
  
  ggplot(sims, aes(x=estimate)) +
    geom_errorbar(aes(ymin=conf.low, ymax=conf.high, color=covered), alpha=.4) +
    geom_hline(aes(yintercept=mean(estimand))) +
    geom_text(aes(x=x, y=y, label=label),
              data=function(df){
                data.frame(x=min(df$estimate),
                           y=mean(df$estimand),
                           label=sprintf('  Avg Estimand:\n  %4.3f', mean(df$estimand)),
                           stringsAsFactors = FALSE)
              }, hjust='left') +
    facet_wrap(estimand_label~estimator_label) +
    ylab("Estimate") +
    scale_x_continuous(labels=NULL, breaks = NULL, name='') +
    scale_color_discrete(drop=FALSE, name = '') +
    coord_flip() +
    dd_theme()
  
```

The figure above may give us the impression that our estimate with clustered standard errors is less precise, when in fact, it correctly accounts for the uncertainty surrounding our estimates. The blue lines in the graph demonstrate the estimates from simulations which contain our estimand. As our table and graphs show, the share of these simulations over the total number of simulations, also known as coverage, is (correctly) close to 95% in estimations with clustered standard errors and 54% in estimations without clustered standard errors. As expected, the mean estimate itself and the bias is the same in both speficications.

## Using the Cluster Sampling Designer

In R, you can generate a cluster sampling design using the template function `cluster_sampling_designer()` in the `DesignLibrary` package by running the following lines, which load the package:

```{r, eval=FALSE}
library(DesignLibrary)
```

We can then create specific designs by defining values for each argument. For example, we create a design called `my_cluster_sampling_design` with `n_clusters`, `n_subjects_per_cluster`, `icc`, `N_clusters`, and `N_subjects_per_cluster` set to 40, 20, .2, 1000, and 50, respectively, by running the lines below.

```{r, eval=FALSE}
my_cluster_sampling_design <- cluster_sampling_designer(n_clusters = 40,
                                                        n_subjects_per_cluster = 20,
                                                        icc = .2, 
                                                        N_clusters = 1000, 
                                                        N_subjects_per_cluster = 50)
```

You can see more details on the `cluster_sampling_designer()` function and its arguments by running the following line of code:

```{r, eval=FALSE}
??cluster_sampling_designer
```

## Further reading